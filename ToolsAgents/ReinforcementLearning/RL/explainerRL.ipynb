{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871fdbd",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook demonstrates Reinforcement Learning fundamentals using a simple\n",
    "LLM-based adaptive agent.\n",
    "\n",
    "We will explicitly see:\n",
    "- State\n",
    "- Action\n",
    "- Reward\n",
    "- Policy\n",
    "- Learning\n",
    "- Next Action\n",
    "\n",
    "⚠️ No model retraining is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716daff9",
   "metadata": {},
   "source": [
    "## One Liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy import ResponsePolicy\n",
    "from rl_state import RLState\n",
    "from prompt import build_prompt\n",
    "from llm import llm, evaluator_llm\n",
    "from feedback_interpreter import interpret_feedback\n",
    "from policy_adapter import apply_policy_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2e717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Policy: {'verbosity': 'medium', 'tone': 'neutral'}\n",
      "Initial State: {'step': 0, 'last_action': None, 'last_reward': None}\n"
     ]
    }
   ],
   "source": [
    "# Initialize RL Components (State & Policy)\n",
    "policy = ResponsePolicy()\n",
    "state = RLState()\n",
    "\n",
    "print(\"Initial Policy:\", policy.as_dict())\n",
    "print(\"Initial State:\", state.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be6fb7",
   "metadata": {},
   "source": [
    "This is our agent before it has taken any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc59c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Input (Question): Explain Reinforcement Learning\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Explain Reinforcement Learning\"\n",
    "print(\"Environment Input (Question):\", user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66308273",
   "metadata": {},
   "source": [
    "#### ACTION: Agent Generates a Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33b79c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response:\n",
      "\n",
      "Reinforcement Learning (RL) is a branch of machine learning focused on how agents can take actions in an environment to maximize cumulative rewards. It is inspired by behavioral psychology and involves learning through interactions with the environment. Here are the key components of reinforcement learning:\n",
      "\n",
      "1. **Agent**: The learner or decision-maker that interacts with the environment.\n",
      "\n",
      "2. **Environment**: The world with which the agent interacts. It provides feedback based on the actions taken by the agent.\n",
      "\n",
      "3. **Actions**: The set of all possible moves the agent can make in the environment.\n",
      "\n",
      "4. **States**: The different situations or configurations of the environment at any given time.\n",
      "\n",
      "5. **Rewards**: Feedback from the environment in the form of numerical values that indicate the desirability of the agent's actions. The goal of the agent is to maximize the total reward over time.\n",
      "\n",
      "6. **Policy**: A strategy or mapping from states to actions that defines the agent's behavior. It can be deterministic or stochastic.\n",
      "\n",
      "7. **Value Function**: A function that estimates how good a particular state or state-action pair is, in terms of expected future rewards. It helps the agent evaluate the long-term benefits of its actions.\n",
      "\n",
      "The learning process in reinforcement learning typically involves the agent exploring the environment, taking actions, receiving rewards, and updating its policy based on the feedback received. This exploration-exploitation trade-off is crucial; the agent must balance exploring new actions to find better rewards and exploiting known actions that yield high rewards.\n",
      "\n",
      "Common algorithms used in reinforcement learning include Q-learning, deep Q-networks (DQN), and policy gradient methods. These algorithms can be applied to a wide range of problems, from game playing (like AlphaGo) to robotics and autonomous systems. \n",
      "\n",
      "Overall, reinforcement learning is a powerful framework for solving complex decision-making problems where the optimal solution is not known in advance and must be learned through trial and error.\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(policy)\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": user_query}).content\n",
    "print(\"Agent Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21848444",
   "metadata": {},
   "source": [
    "#### This is Action aₜ — chosen using the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2a399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Taken (Behavior Choice): {'verbosity': 'medium', 'tone': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "action = policy.as_dict()\n",
    "print(\"Action Taken (Behavior Choice):\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d2518",
   "metadata": {},
   "source": [
    "In RL, the action is not the text — it’s the behavior choice\n",
    "\n",
    "Record the Action Taken (Policy → Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c0ec5",
   "metadata": {},
   "source": [
    "### ENVIRONMENT FEEDBACK (Human Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43abfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Feedback: too short\n"
     ]
    }
   ],
   "source": [
    "feedback = \"too short\"\n",
    "print(\"Environment Feedback:\", feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942074",
   "metadata": {},
   "source": [
    "### REWARD: Interpret Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8085ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback Interpretation:\n",
      "{'reward': -1, 'dimensions': {'verbosity': 'increase', 'tone': 'no_change'}}\n",
      "\n",
      "Reward Signal: -1\n"
     ]
    }
   ],
   "source": [
    "interpretation = interpret_feedback(feedback, evaluator_llm)\n",
    "reward = interpretation[\"reward\"]\n",
    "\n",
    "print(\"Feedback Interpretation:\")\n",
    "print(interpretation)\n",
    "print(\"\\nReward Signal:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e950",
   "metadata": {},
   "source": [
    "### LEARNING: Update Policy Using Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588735c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy BEFORE learning: {'verbosity': 'medium', 'tone': 'neutral'}\n",
      "Policy AFTER learning: {'verbosity': 'detailed', 'tone': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy BEFORE learning:\", policy.as_dict())\n",
    "\n",
    "learning_result = apply_policy_update(policy, interpretation)\n",
    "\n",
    "print(\"Policy AFTER learning:\", policy.as_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f42c6b",
   "metadata": {},
   "source": [
    "### STATE TRANSITION (sₜ → sₜ₊₁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69589fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated RL State:\n",
      "{'step': 1, 'last_action': {'verbosity': 'medium', 'tone': 'neutral'}, 'last_reward': -1}\n"
     ]
    }
   ],
   "source": [
    "state.update(\n",
    "    action=action,\n",
    "    reward=reward\n",
    ")\n",
    "\n",
    "print(\"Updated RL State:\")\n",
    "print(state.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3efef",
   "metadata": {},
   "source": [
    "### Re-run Agent with Updated Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f2f2def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Agent Response After Learning:\n",
      "\n",
      "Reinforcement Learning (RL) is a subfield of machine learning concerned with how agents ought to take actions in an environment in order to maximize some notion of cumulative reward. It is inspired by behavioral psychology, where learning is thought to occur through interactions with the environment, leading to changes in behavior based on feedback from those interactions.\n",
      "\n",
      "Here are the fundamental concepts and components of reinforcement learning:\n",
      "\n",
      "1. **Agent**: The learner or decision maker that interacts with the environment. The agent takes actions based on its observations and aims to achieve a goal.\n",
      "\n",
      "2. **Environment**: Everything that the agent interacts with. The environment can be defined as a set of states, actions, rewards, and transitions that the agent navigates through.\n",
      "\n",
      "3. **State (s)**: A representation of the current situation of the agent in the environment. The state provides all the necessary information that the agent needs to decide what to do next.\n",
      "\n",
      "4. **Action (a)**: A choice made by the agent that affects the state of the environment. The set of all possible actions is often referred to as the action space.\n",
      "\n",
      "5. **Reward (r)**: A scalar feedback signal received by the agent after taking an action in a particular state. The reward serves as a measure of the immediate benefit of an action and is crucial for guiding the learning process.\n",
      "\n",
      "6. **Policy (π)**: A strategy used by the agent to decide which action to take given a particular state. A policy can be deterministic (a specific action for each state) or stochastic (a probability distribution over actions for each state).\n",
      "\n",
      "7. **Value Function (V)**: A function that estimates the expected cumulative reward that an agent can obtain from a given state, following a certain policy. It helps the agent evaluate the long-term benefit of being in a particular state.\n",
      "\n",
      "8. **Q-Value (Q)**: Also known as the action-value function, it measures the expected cumulative reward of taking a specific action in a specific state and then following a certain policy thereafter.\n",
      "\n",
      "The learning process in reinforcement learning typically involves the following steps:\n",
      "\n",
      "1. **Exploration vs. Exploitation**: The agent must balance exploring new actions to discover their effects (exploration) and exploiting known actions that yield high rewards (exploitation). This trade-off is central to the learning process.\n",
      "\n",
      "2. **Learning from Interaction**: The agent interacts with the environment by taking actions, observing the resulting states and rewards, and updating its policy or value functions based on this feedback.\n",
      "\n",
      "3. **Temporal Difference Learning**: Many RL algorithms use temporal difference methods, which allow the agent to learn about the value of states based on the rewards received over time, even before the final outcome is known.\n",
      "\n",
      "4. **Training and Convergence**: Over time, as the agent gathers more experiences and updates its policy, it converges towards an optimal policy, which maximizes the expected cumulative reward in the long term.\n",
      "\n",
      "Reinforcement learning has practical applications in various fields, including robotics, game playing (such as AlphaGo), autonomous vehicles, finance, and healthcare. It is particularly powerful in situations where the optimal decision-making strategy is not known in advance and must be learned through trial and error.\n",
      "\n",
      "In summary, reinforcement learning is a powerful framework for learning how to make decisions in complex environments, where the outcomes of actions are uncertain, and feedback is provided in the form of rewards. Its combination of exploration, exploitation, and learning from interactions makes it a versatile and widely applicable approach in artificial intelligence and machine learning.\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(policy)\n",
    "chain = prompt | llm\n",
    "\n",
    "new_response = chain.invoke({\"input\": user_query}).content\n",
    "\n",
    "print(\"New Agent Response After Learning:\\n\")\n",
    "print(new_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ba763",
   "metadata": {},
   "source": [
    "### Full RL Loop Summary (Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0fe12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {'step': 1,\n",
       "  'last_action': {'verbosity': 'medium', 'tone': 'neutral'},\n",
       "  'last_reward': -1},\n",
       " 'policy': {'verbosity': 'detailed', 'tone': 'neutral'},\n",
       " 'last_reward': -1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    \"state\": state.as_dict(),\n",
    "    \"policy\": policy.as_dict(),\n",
    "    \"last_reward\": reward\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700621d7",
   "metadata": {},
   "source": [
    "### RL Mapping\n",
    "\n",
    "| RL Concept | Where it appears |\n",
    "|-----------|------------------|\n",
    "| State     | RLState |\n",
    "| Action    | Policy-driven behavior |\n",
    "| Reward    | Interpreted feedback |\n",
    "| Policy    | ResponsePolicy |\n",
    "| Learning  | apply_policy_update |\n",
    "| Environment | User + feedback |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f3447",
   "metadata": {},
   "source": [
    "## When DOES your system become RLHF?\n",
    "\n",
    "#### 1. You introduce a learned reward model\n",
    "Example:\n",
    "* Train a small model to score responses\n",
    "* That model replaces direct interpretation logic\n",
    "\n",
    "#### 2. You collect preference comparisons\n",
    "\n",
    "Example:\n",
    "* “Which response do you prefer? A or B?”\n",
    "* Reward is derived from preference ranking\n",
    "\n",
    "#### 3. You optimize policy using PPO-like updates\n",
    "\n",
    "Example:\n",
    "* Policy is updated to maximize expected reward over time\n",
    "* Not just rule-based updates\n",
    "\n",
    "\n",
    "\n",
    "### RLHF is a special case of RL — not a different paradigm NOT Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ac06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43de7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
