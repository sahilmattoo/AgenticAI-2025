{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871fdbd",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This notebook demonstrates Reinforcement Learning fundamentals using a simple\n",
    "LLM-based adaptive agent.\n",
    "\n",
    "We will explicitly see:\n",
    "- State\n",
    "- Action\n",
    "- Reward\n",
    "- Policy\n",
    "- Learning\n",
    "- Next Action\n",
    "\n",
    "⚠️ No model retraining is involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716daff9",
   "metadata": {},
   "source": [
    "## One Liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0ae6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policy import ResponsePolicy\n",
    "from rl_state import RLState\n",
    "from prompt import build_prompt\n",
    "from llm import llm, evaluator_llm\n",
    "from feedback_interpreter import interpret_feedback\n",
    "from policy_adapter import apply_policy_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2e717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Policy: {'verbosity': 'medium', 'tone': 'neutral'}\n",
      "Initial State: {'step': 0, 'last_action': None, 'last_reward': None}\n"
     ]
    }
   ],
   "source": [
    "# Initialize RL Components (State & Policy)\n",
    "policy = ResponsePolicy()\n",
    "state = RLState()\n",
    "\n",
    "print(\"Initial Policy:\", policy.as_dict())\n",
    "print(\"Initial State:\", state.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be6fb7",
   "metadata": {},
   "source": [
    "This is our agent before it has taken any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc59c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Input (Question): Explain Reinforcement Learning\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Explain Reinforcement Learning\"\n",
    "print(\"Environment Input (Question):\", user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66308273",
   "metadata": {},
   "source": [
    "#### ACTION: Agent Generates a Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b79c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Response:\n",
      "\n",
      "Reinforcement Learning (RL) is a branch of machine learning where an agent learns to make decisions by interacting with an environment. The core idea is to enable the agent to learn optimal behaviors through trial and error, receiving feedback in the form of rewards or penalties based on its actions.\n",
      "\n",
      "Here are some key components of Reinforcement Learning:\n",
      "\n",
      "1. **Agent**: The learner or decision maker that interacts with the environment.\n",
      "\n",
      "2. **Environment**: Everything the agent interacts with. The environment responds to the agent's actions and provides feedback.\n",
      "\n",
      "3. **State**: A representation of the current situation of the agent within the environment. The state can change based on the actions taken by the agent.\n",
      "\n",
      "4. **Action**: The choices available to the agent that can affect the state of the environment.\n",
      "\n",
      "5. **Reward**: A scalar feedback signal received after taking an action in a particular state. The reward indicates how good or bad the action was in achieving the desired outcome.\n",
      "\n",
      "6. **Policy**: A strategy used by the agent to determine the next action based on the current state. The policy can be deterministic or stochastic.\n",
      "\n",
      "7. **Value Function**: A function that estimates the expected return or future rewards from a given state, helping the agent understand the long-term value of states and actions.\n",
      "\n",
      "The learning process typically involves the following steps:\n",
      "\n",
      "1. **Exploration vs. Exploitation**: The agent must balance exploring new actions to discover their effects (exploration) and utilizing known actions that yield high rewards (exploitation).\n",
      "\n",
      "2. **Learning from Interaction**: The agent takes actions in the environment, observes the resulting states and rewards, and uses this information to update its policy or value function.\n",
      "\n",
      "3. **Goal**: The ultimate goal of the agent is to maximize the cumulative reward over time, effectively finding an optimal policy that leads to the best long-term outcomes.\n",
      "\n",
      "Reinforcement Learning has applications in various fields, such as robotics, game playing (e.g., AlphaGo), autonomous vehicles, finance, and healthcare, among others. It is particularly useful in scenarios where the decision-making problem is complex and the environment may be dynamic or partially observable.\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(policy)\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": user_query}).content\n",
    "print(\"Agent Response:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21848444",
   "metadata": {},
   "source": [
    "#### This is Action aₜ — chosen using the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2a399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Taken (Behavior Choice): {'verbosity': 'medium', 'tone': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "action = policy.as_dict()\n",
    "print(\"Action Taken (Behavior Choice):\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d2518",
   "metadata": {},
   "source": [
    "In RL, the action is not the text — it’s the behavior choice\n",
    "\n",
    "Record the Action Taken (Policy → Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c0ec5",
   "metadata": {},
   "source": [
    "### ENVIRONMENT FEEDBACK (Human Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43abfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Feedback: too short\n"
     ]
    }
   ],
   "source": [
    "feedback = \"too short\"\n",
    "print(\"Environment Feedback:\", feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d942074",
   "metadata": {},
   "source": [
    "### REWARD: Interpret Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8085ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback Interpretation:\n",
      "{'reward': -1, 'dimensions': {'verbosity': 'increase', 'tone': 'no_change'}}\n",
      "\n",
      "Reward Signal: -1\n"
     ]
    }
   ],
   "source": [
    "interpretation = interpret_feedback(feedback, evaluator_llm)\n",
    "reward = interpretation[\"reward\"]\n",
    "\n",
    "print(\"Feedback Interpretation:\")\n",
    "print(interpretation)\n",
    "print(\"\\nReward Signal:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891e950",
   "metadata": {},
   "source": [
    "### LEARNING: Update Policy Using Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588735c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy BEFORE learning: {'verbosity': 'medium', 'tone': 'neutral'}\n",
      "Policy AFTER learning: {'verbosity': 'detailed', 'tone': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy BEFORE learning:\", policy.as_dict())\n",
    "\n",
    "learning_result = apply_policy_update(policy, interpretation)\n",
    "\n",
    "print(\"Policy AFTER learning:\", policy.as_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f42c6b",
   "metadata": {},
   "source": [
    "### STATE TRANSITION (sₜ → sₜ₊₁)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69589fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated RL State:\n",
      "{'step': 1, 'last_action': {'verbosity': 'medium', 'tone': 'neutral'}, 'last_reward': -1}\n"
     ]
    }
   ],
   "source": [
    "state.update(\n",
    "    action=action,\n",
    "    reward=reward\n",
    ")\n",
    "\n",
    "print(\"Updated RL State:\")\n",
    "print(state.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3efef",
   "metadata": {},
   "source": [
    "### Re-run Agent with Updated Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2f2def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Agent Response After Learning:\n",
      "\n",
      "Reinforcement Learning (RL) is a subfield of machine learning focused on how agents should take actions in an environment to maximize a cumulative reward. It is inspired by behavioral psychology and the concept of learning from interaction. The fundamental idea is that an agent learns to achieve a goal in an uncertain, potentially complex environment by interacting with it and receiving feedback in the form of rewards or penalties.\n",
      "\n",
      "### Key Concepts in Reinforcement Learning:\n",
      "\n",
      "1. **Agent**: The learner or decision maker that interacts with the environment. The agent takes actions based on its current knowledge and the state of the environment.\n",
      "\n",
      "2. **Environment**: Everything the agent interacts with. The environment provides feedback based on the actions taken by the agent.\n",
      "\n",
      "3. **State (s)**: A representation of the current situation of the agent within the environment. The state can contain all necessary information for the agent to make a decision.\n",
      "\n",
      "4. **Action (a)**: The set of all possible moves the agent can make at any given state. The agent chooses actions based on its policy.\n",
      "\n",
      "5. **Policy (π)**: A strategy used by the agent to determine the next action based on the current state. The policy can be deterministic (always choosing the same action for a given state) or stochastic (choosing actions based on a probability distribution).\n",
      "\n",
      "6. **Reward (r)**: A scalar feedback signal received after taking an action in a particular state. The reward indicates how good or bad the action was in terms of achieving the goal.\n",
      "\n",
      "7. **Return (G)**: The total accumulated reward the agent receives over time. It is often calculated as a sum of discounted future rewards, where the discount factor (γ) determines the importance of future rewards compared to immediate rewards.\n",
      "\n",
      "8. **Value Function (V)**: A function that estimates the expected return (or future rewards) from a given state, under a particular policy. It helps the agent evaluate how good it is to be in a certain state.\n",
      "\n",
      "9. **Q-Function (Q)**: A function that estimates the expected return from taking a specific action in a given state, and then following a particular policy. It provides more detailed information than the value function.\n",
      "\n",
      "### Reinforcement Learning Process:\n",
      "\n",
      "The RL process generally follows these steps:\n",
      "\n",
      "1. **Initialization**: The agent initializes its policy and value functions. This can vary from random initialization to more informed initial estimates.\n",
      "\n",
      "2. **Interaction with the Environment**:\n",
      "   - The agent observes the current state of the environment.\n",
      "   - Based on its policy, the agent selects an action to perform.\n",
      "   - The agent executes the action, and the environment responds with a new state and a reward.\n",
      "\n",
      "3. **Learning**: The agent updates its policy and value functions based on the feedback (reward) received. This can be done using various algorithms and methods, such as Q-learning, SARSA, or policy gradient methods.\n",
      "\n",
      "4. **Iteration**: The process repeats, allowing the agent to refine its policy over time through exploration (trying new actions) and exploitation (choosing actions that are known to yield high rewards).\n",
      "\n",
      "### Exploration vs. Exploitation:\n",
      "\n",
      "A crucial aspect of reinforcement learning is the trade-off between exploration and exploitation. Exploration involves trying new actions to discover their effects, while exploitation involves making the best decision based on current knowledge. Balancing the two is essential for effective learning.\n",
      "\n",
      "### Common Algorithms in Reinforcement Learning:\n",
      "\n",
      "1. **Q-Learning**: A model-free algorithm that learns the value of actions directly. It updates Q-values based on the Bellman equation.\n",
      "\n",
      "2. **SARSA (State-Action-Reward-State-Action)**: Similar to Q-learning, but it updates the Q-value based on the action actually taken, which can lead to different behaviors.\n",
      "\n",
      "3. **Deep Q-Networks (DQN)**: A variant of Q-learning that uses deep neural networks to approximate Q-values, allowing it to handle high-dimensional state spaces.\n",
      "\n",
      "4. **Policy Gradient Methods**: These methods learn a parameterized policy directly, optimizing the policy using gradients based on the expected return.\n",
      "\n",
      "5. **Actor-Critic Methods**: These methods combine both value-based and policy-based approaches, where the \"actor\" updates the policy and the \"critic\" evaluates the policy by estimating value functions.\n",
      "\n",
      "### Applications of Reinforcement Learning:\n",
      "\n",
      "Reinforcement Learning has a wide range of applications, including:\n",
      "\n",
      "- Robotics: Teaching robots to perform tasks through trial and error.\n",
      "- Game Playing: RL has been successfully used in games like Go, Chess, and various video games.\n",
      "- Autonomous Vehicles: Allowing vehicles to learn safe navigation and decision-making.\n",
      "- Recommendation Systems: Optimizing recommendations based on user interactions.\n",
      "- Finance: Developing trading strategies that adapt to market changes.\n",
      "\n",
      "In summary, Reinforcement Learning is a powerful framework for learning optimal behaviors through interaction with an environment, driven by the feedback of rewards, and it has broad applications across various domains.\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(policy)\n",
    "chain = prompt | llm\n",
    "\n",
    "new_response = chain.invoke({\"input\": user_query}).content\n",
    "\n",
    "print(\"New Agent Response After Learning:\\n\")\n",
    "print(new_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ba763",
   "metadata": {},
   "source": [
    "### Full RL Loop Summary (Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c0fe12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {'step': 1,\n",
       "  'last_action': {'verbosity': 'medium', 'tone': 'neutral'},\n",
       "  'last_reward': -1},\n",
       " 'policy': {'verbosity': 'detailed', 'tone': 'neutral'},\n",
       " 'last_reward': -1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    \"state\": state.as_dict(),\n",
    "    \"policy\": policy.as_dict(),\n",
    "    \"last_reward\": reward\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700621d7",
   "metadata": {},
   "source": [
    "### RL Mapping\n",
    "\n",
    "| RL Concept | Where it appears |\n",
    "|-----------|------------------|\n",
    "| State     | RLState |\n",
    "| Action    | Policy-driven behavior |\n",
    "| Reward    | Interpreted feedback |\n",
    "| Policy    | ResponsePolicy |\n",
    "| Learning  | apply_policy_update |\n",
    "| Environment | User + feedback |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f3447",
   "metadata": {},
   "source": [
    "## When DOES your system become RLHF?\n",
    "\n",
    "#### 1. You introduce a learned reward model\n",
    "Example:\n",
    "* Train a small model to score responses\n",
    "* That model replaces direct interpretation logic\n",
    "\n",
    "#### 2. You collect preference comparisons\n",
    "\n",
    "Example:\n",
    "* “Which response do you prefer? A or B?”\n",
    "* Reward is derived from preference ranking\n",
    "\n",
    "#### 3. You optimize policy using PPO-like updates\n",
    "\n",
    "Example:\n",
    "* Policy is updated to maximize expected reward over time\n",
    "* Not just rule-based updates\n",
    "\n",
    "\n",
    "\n",
    "### RLHF is a special case of RL — not a different paradigm NOT Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ac06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eea976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43de7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
