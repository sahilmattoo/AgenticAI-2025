{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94bf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934f03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54b47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables.\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded environment variables.\")\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14092f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7688191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a quiet valley where the stars shone brighter than anywhere else, there lived a small unicorn named Liora. Her coat was soft and silver, and her horn glowed with the palest blue light, like moonlight on snow.\n",
      "\n",
      "Every unicorn in the valley had a special gift. Some could make flowers bloom with a stomp of their hooves, others could sing songs that made rainbows appear. Liora, however, did not yet know what her gift was.\n",
      "\n",
      "Each night, she watched the older unicorns practice their magic. She tried to help: she stomped her hooves over the grass, but no flowers came. She sang softly to the sky, but no rainbows formed. She dipped her horn into the river, hoping it would turn to sparkling sugar, but the water stayed water.\n",
      "\n",
      "One evening, feeling a little sad, Liora wandered away from the valley and climbed a small hill. Above her, the sky was dark and deep, sprinkled with thousands of tiny stars. The moon hung low and round, like a silver lantern.\n",
      "\n",
      "“I wish I knew what I’m meant to do,” she whispered.\n",
      "\n",
      "The wind rustled the grass in answer, gentle and cool. Liora lay down and gazed up, her horn glowing faintly. As she watched, she noticed something new: a star, dim and flickering at the edge of the sky, as if it were too tired to shine.\n",
      "\n",
      "“Oh,” she breathed. “You look lonely, too.”\n",
      "\n",
      "She stood up and pointed her horn toward the little star. Her horn glowed brighter, then brighter still, sending a slender beam of soft blue light into the sky. The light touched the faint star, wrapping it in a gentle glow.\n",
      "\n",
      "The star quivered, then grew a little stronger. It twinkled back at her, as if saying thank you.\n",
      "\n",
      "Liora’s heart fluttered. “Did I do that?”\n",
      "\n",
      "She tried again, focusing on another tiny, sleepy star. Her horn warmed and hummed, and once more, a ray of light reached up. The second star brightened, then the first one sparkled even more, as though they were waking each other up.\n",
      "\n",
      "All around her, the sky slowly changed. The dimmest stars began to glow, then to shine, until the whole night seemed to grow deeper and more beautiful. The valley behind her filled with a cool, gentle light, and the other unicorns looked up in wonder.\n",
      "\n",
      "“Who is brightening the stars?” they murmured.\n",
      "\n",
      "High on the little hill, Liora’s horn blazed softly like a lantern. She wasn’t forcing the stars to glow; she was simply reminding them of their own light. One by one, they answered her, twinkling awake, until the sky was a blanket of sparkling silver.\n",
      "\n",
      "A soft voice drifted through the night, as if carried on the breeze. “Liora,” it said, “your gift is to help the smallest lights shine.”\n",
      "\n",
      "The voice felt like moonlight itself, and Liora understood. She didn’t need to grow flowers or sing rainbows. Her magic was quieter: she made the night less lonely, the darkness less dark, by helping every hidden sparkle find its glow.\n",
      "\n",
      "When her horn finally dimmed, the stars stayed bright. The valley below shimmered with starlight, and the unicorns lay down to sleep, comforted by the soft, steady glow above them.\n",
      "\n",
      "Liora curled up on her hill, the moon smiling down. She no longer wondered what her gift was; she had found it among the stars.\n",
      "\n",
      "And as she closed her eyes, tiny new stars—ones no one had ever noticed before—winked into view, shining gently over the world, keeping watch through the quiet, peaceful night.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5.1\",\n",
    "    input=\"Write a short bedtime story about a unicorn.\"\n",
    ")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bacc63",
   "metadata": {},
   "source": [
    "## Test Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4585a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text='Hi there, this is a regenerated audio sample for testing the OpenAI transcription API. This audio is short, clear, and suitable for your transcription experiments.', logprobs=None, usage=UsageTokens(input_tokens=88, output_tokens=33, total_tokens=121, type='tokens', input_token_details=UsageTokensInputTokenDetails(audio_tokens=88, text_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"speech_test_regenerated.mp3\", \"rb\")\n",
    "\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6edfa",
   "metadata": {},
   "source": [
    "# GROQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04303a8",
   "metadata": {},
   "source": [
    "### Set the key in .env\n",
    "### load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bc382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b039b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd14fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there anything I can help you with?"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004f96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since my knowledge cutoff is December 2023, I'll provide information on AI research news from the period leading up to that date. However, please note that some of the information might have changed or been updated since then.\n",
      "\n",
      "Here are some of the latest developments in AI research from the past few months (up to December 2023):\n",
      "\n",
      "1. **Breakthroughs in Large Language Models:** Researchers have made significant progress in developing more efficient and accurate large language models (LLMs). For example, Meta AI's LLaMA model has shown excellent performance in various NLP tasks, and the researchers have also made improvements to the model architecture.\n",
      "\n",
      "2. **Advances in Explainable AI (XAI):** Researchers at Google have made strides in developing more interpretable and explainable AI models. They have created a new tool, Explainable Neural Networks (ENN), which can provide human-interpretable explanations for AI decisions.\n",
      "\n",
      "3. **AI for Cybersecurity:** Researchers at the University of California, Berkeley have developed a new AI-based system for detecting and preventing cyber attacks. The system uses machine learning algorithms to identify and block malicious traffic in real-time.\n",
      "\n",
      "4. **Advances in Reinforcement Learning:** Researchers at DeepMind have made significant breakthroughs in reinforcement learning (RL) techniques. They have developed new RL algorithms that can learn to solve complex tasks, such as robotic manipulation and game playing.\n",
      "\n",
      "5. **Quantum AI Research:** Researchers at the Massachusetts Institute of Technology (MIT) have started exploring the intersection of quantum computing and AI. They have developed new AI algorithms that can run on quantum hardware and potentially solve complex problems more efficiently than classical AI.\n",
      "\n",
      "6. **Ethics in AI Research:** The Association for the Advancement of Artificial Intelligence (AAAI) has released guidelines for addressing the ethics of AI research. The guidelines aim to promote transparency, accountability, and fairness in AI development.\n",
      "\n",
      "Please note that AI research is a rapidly evolving field, and there may be more recent developments that have not been included in this list.\n",
      "\n",
      "Please keep in mind, since my knowledge stopped in December, 2023. AI advancements could be much faster than what was reported."
     ]
    }
   ],
   "source": [
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Share the latest news about AI research from the past month.\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ba2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from ollama) (2.12.4)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae013e7",
   "metadata": {},
   "source": [
    "# deepseek-r1:1.5b Using OLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87d2ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky, known as coloration or chromacy, is primarily determined by the amount of sunlight reflected on the Earth's surface. Different parts of the sky have varying levels of this reflection because light reflecting off water particles in the atmosphere, such as clouds and mist, can pass through more air molecules than light from objects like buildings or cars that reflects off dust or other particles. This difference in reflection leads to slight variations in the hues we perceive when looking up at the sky. Additionally, atmospheric conditions, including temperature, humidity, and the presence of trace gases like carbon dioxide, influence how the sky appears.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "#print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370be03",
   "metadata": {},
   "source": [
    "# Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbdf6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc581ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI refers to artificial intelligence systems that possess the ability to take autonomous actions, make decisions, and interact with the environment to achieve specific goals or tasks. Unlike traditional AI, which often operates under human direction and focuses on tasks like data analysis or pattern recognition, agentic AI can operate independently and adaptively.\\n\\nKey characteristics of agentic AI may include:\\n\\n1. **Autonomy**: The AI can perform tasks and make decisions without human intervention.\\n2. **Goal-oriented behavior**: It is designed to achieve specific objectives or goals, which could involve planning and decision-making.\\n3. **Adaptability**: Agentic AI can learn from its experiences and change its behavior based on new information or feedback from its environment.\\n4. **Interactivity**: These systems can often interact with complex environments or with humans and other agents to achieve their objectives.\\n\\nExamples of agentic AI can range from advanced robotics (such as autonomous drones or self-driving cars) to software agents in complex computational environments (like trading bots in financial markets).\\n\\nThe development and deployment of agentic AI raise various ethical and safety considerations, particularly regarding accountability, decision-making transparency, and the potential for unintended consequences. Organizations and researchers are actively exploring frameworks and guidelines to ensure that agentic AI systems are developed and used in responsible and beneficial ways.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 263, 'prompt_tokens': 23, 'total_tokens': 286, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b547601dbd', 'id': 'chatcmpl-CgusnSgmeatiONwJPioewSz9q07QI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--411c3209-dbcc-48c9-ac03-97491227e3e0-0' usage_metadata={'input_tokens': 23, 'output_tokens': 263, 'total_tokens': 286, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"question\": \"What is Agentic AI?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1781e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1daaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Transformers are a type of artificial neural network (ANN) that\\'s particularly good at processing sequences of data, like words in a sentence or pixels in an image. They\\'re called \"transformers\" because they apply transformations to the input data to get the output.\\n\\nHere\\'s a simplified explanation of how they work:\\n\\n1. **Input**: The transformer takes a sequence of data, like a sentence or an image, as input.\\n2. **Tokenization**: The input data is broken down into smaller pieces called tokens (e.g., words or image patches).\\n3. **Encoding**: Each token is converted into a numerical vector, called an embedding, that represents its meaning.\\n4. **Self-Attention**: The transformer applies a mechanism called self-attention to the embedded tokens. It looks at each token and asks, \"What\\'s the most relevant information from the other tokens that I need to understand this one?\"\\n5. **Transformation**: Based on the answers to the above question, the transformer applies a transformation to each token to generate a new representation.\\n6. **Output**: The transformed tokens are combined to produce the final output, like a predicted sentence or a classified image.\\n\\nThe key innovation of transformers is the self-attention mechanism, which allows them to focus on relevant parts of the input data and ignore irrelevant parts. This makes them particularly good at tasks like:\\n\\n* Language translation\\n* Text summarization\\n* Image classification\\n* Question answering\\n\\nTransformers are a powerful tool in natural language processing (NLP) and computer vision, and have been widely used in many applications, including chatbots, virtual assistants, and image recognition systems.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 331, 'prompt_tokens': 42, 'total_tokens': 373, 'completion_time': 0.523423273, 'completion_tokens_details': None, 'prompt_time': 0.002474567, 'prompt_tokens_details': None, 'queue_time': 0.049724363, 'total_time': 0.52589784}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--4d02be15-12cb-439c-85c1-4ba509d619fe-0' usage_metadata={'input_tokens': 42, 'output_tokens': 331, 'total_tokens': 373}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "response = llm.invoke(\"Explain transformers in simple words.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30faaa",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb5311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain embeddings in 3 bullet points.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in 3 bullet points.\"\n",
    ")\n",
    "\n",
    "print(template.format(topic=\"embeddings\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcbe49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'Apple', 'revenue': '$383B'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Extract company name and revenue from the text below.\n",
    "Return output ONLY in JSON format with keys company, revenue.\n",
    "\n",
    "Text:\n",
    "Apple generated $383B revenue in 2023.\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI()\n",
    "print(parser.parse(model.invoke(prompt).content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11425088",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314370aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 embedding length: 1536\n",
      "[0.015356769785284996, 0.03452569618821144, 0.03314683213829994, 0.01639767736196518, -0.021115558221936226, -0.0155595438554883, 0.013437173329293728, 0.012734223157167435, -0.018749859184026718, 0.019087815657258034] ...\n",
      "Text 2 embedding length: 1536\n",
      "[-0.027849717065691948, 0.003486940171569586, 0.04356098920106888, 0.0014729317044839263, 0.014623391442000866, -0.01214989647269249, 0.015505146235227585, -0.00042477401439100504, 0.002828486729413271, 0.023143205791711807] ...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Example text list\n",
    "texts = [\n",
    "    \"Retrieval-Augmented Generation (RAG) improves accuracy.\",\n",
    "    \"Embeddings convert text into high-dimensional vectors.\"\n",
    "]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=texts\n",
    ")\n",
    "\n",
    "# Print embeddings\n",
    "for i, emb in enumerate(response.data):\n",
    "    print(f\"Text {i+1} embedding length:\", len(emb.embedding))\n",
    "    print(emb.embedding[:10], \"...\")  # show first 10 numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cd81210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "268a5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c468265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "- Transfer learning speeds up deep learning training.\n",
      "- RAG improves retrieval accuracy for enterprise systems.\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 2. Sample documents\n",
    "docs = [\n",
    "    \"India won its Test match by 200 runs.\",\n",
    "    \"RAG improves retrieval accuracy for enterprise systems.\",\n",
    "    \"Transfer learning speeds up deep learning training.\"\n",
    "]\n",
    "\n",
    "# Step 3. Create vector store\n",
    "faiss_index = FAISS.from_texts(docs, embedding_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c45063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "- Transfer learning speeds up deep learning training.\n",
      "- RAG improves retrieval accuracy for enterprise systems.\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Search\n",
    "query = \"How do I improve AI accuracy?\"\n",
    "results = faiss_index.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Top results:\")\n",
    "for r in results:\n",
    "    print(\"-\", r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2813f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb7cd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12a452bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing FAISS index\n",
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35087d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\n",
    "    \"AI can automate repetitive tasks.\",\n",
    "    \"Machine learning models are improving healthcare outcomes.\",\n",
    "    \"The stock market crashed yesterday due to global tension.\",\n",
    "    \"Cats are very playful animals.\",\n",
    "    \"Python is widely used in data science.\",\n",
    "    \"Electric vehicles are becoming more popular.\",\n",
    "    \"The weather today is sunny with mild winds.\",\n",
    "    \"Football is the most popular sport in the world.\",\n",
    "    \"Quantum computing is the future of cryptography.\",\n",
    "    \"Deep learning helps in image recognition.\",\n",
    "    \"The restaurant serves Italian and Mexican food.\",\n",
    "    \"Traveling to Japan is on my wishlist.\",\n",
    "    \"Reinforcement learning trains agents via rewards.\",\n",
    "    \"Natural Language Processing enables chatbots.\",\n",
    "    \"The new iPhone was launched with major upgrades.\",\n",
    "    \"Meditation improves mental health.\",\n",
    "    \"Blockchain technology ensures secure transactions.\",\n",
    "    \"Yoga enhances flexibility and strength.\",\n",
    "    \"Cloud computing allows scalable applications.\",\n",
    "    \"Climate change impacts global ecosystems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49e28f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fedbf86f-e0f3-4aa6-82a8-9438d174773f',\n",
       " '64fba312-22a9-440d-921e-1c029df9593d',\n",
       " '892017d6-430c-4215-abfc-e7dd386749f9',\n",
       " 'bbb04f95-0d53-4a0c-b4b0-dd6813cac7c9',\n",
       " 'dedf16ef-62e1-46af-b8b4-b0a663ca0d31',\n",
       " 'adc02ff3-8444-4583-9306-ab0a636b2f56',\n",
       " '55cfc178-a97c-41a2-9eff-d259ba1d4613',\n",
       " '5e6cb105-5166-4390-8617-86f0ffcfed28',\n",
       " '59cb40b5-4b3e-4d25-a79d-b95128d3afed',\n",
       " '5a585964-b038-4b3b-8a7d-69775f19282d',\n",
       " '511f5dd6-d287-42c1-a5ab-facff615d06f',\n",
       " '31b7c22b-d3ee-486f-aa7e-dc7f1710ef68',\n",
       " 'ecf6ba3d-a2e2-478f-a817-a4dc98ac4e27',\n",
       " '23b4fdaf-88b3-4977-bc0e-34d03675b16a',\n",
       " '2d26d37c-bcb1-40cc-9523-cd000ecea548',\n",
       " 'c3158f71-c5aa-4f3f-99e8-f4661c5e8e8d',\n",
       " '4c805f06-82ca-4078-ae92-e320e5edbaff',\n",
       " 'b0c43edd-4a34-4866-9bea-5e0594b92177',\n",
       " 'abe38cf7-043e-470a-84c2-ed29fa58ddf6',\n",
       " '0a0be554-5325-468e-a1c5-0f40917ec87e']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_texts(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26516d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated index\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb0e8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top search results:\n",
      "- Machine learning models are improving healthcare outcomes.\n",
      "- AI can automate repetitive tasks.\n",
      "- Deep learning helps in image recognition.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can AI help in healthcare?\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "print(\"\\nTop search results:\")\n",
    "for r in results:\n",
    "    print(\"-\", r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a62c4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: RAG improves retrieval accuracy for enterprise systems.\n",
      "Score: 1.3513801\n",
      "---\n",
      "Text: Cloud computing allows scalable applications.\n",
      "Score: 1.5137949\n",
      "---\n",
      "Text: Transfer learning speeds up deep learning training.\n",
      "Score: 1.5689678\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How do vector databases work?\"\n",
    "\n",
    "results = db.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(\"Text:\", doc.page_content)\n",
    "    print(\"Score:\", score)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5de74c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index type: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"FAISS index type:\", db.index.metric_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335441d",
   "metadata": {},
   "source": [
    "#### distance\n",
    "1 -- Inner Product (higher is better)\n",
    "\n",
    "0 -- L2 Distance (Smaller is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fab01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b35dfa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric type: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "texts = [\n",
    "    \"India won the cricket match.\",\n",
    "    \"FAISS is used for similarity search.\",\n",
    "    \"Embeddings turn text into vectors.\"\n",
    "]\n",
    "\n",
    "db = FAISS.from_texts(\n",
    "    texts,\n",
    "    emb,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "print(\"Metric type:\", db.index.metric_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1968",
   "metadata": {},
   "source": [
    "#### Different Distance\n",
    "    1. DistanceStrategy.EUCLIDEAN (L2)\n",
    "        - L2\n",
    "        - Lower = closer\n",
    "        - Useful for numeric vectors, clustering\n",
    "    2. DistanceStrategy.COSINE\n",
    "        - Equivalent to cosine similarity\n",
    "        - Higher = closer\n",
    "        - Best for RAG, semantic search\n",
    "    3. DistanceStrategy.MAX_INNER_PRODUCT\n",
    "        - Higher = similar\n",
    "        - Good for ranking tasks\n",
    "        - Recommendation Systems (weighted embeddings), Product Ranking on E-commerce Platforms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a20bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Cosine similarity\n",
    "db_cos = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.COSINE)\n",
    "\n",
    "# Euclidean distance\n",
    "db_l2 = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.EUCLIDEAN)\n",
    "\n",
    "# Max inner-product\n",
    "db_ip = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e542d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42593ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
