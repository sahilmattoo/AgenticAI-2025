{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934f03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51858569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54b47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables.\n",
      "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\n"
     ]
    }
   ],
   "source": [
    "print(\"Loaded environment variables.\")\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14092f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = OpenAI()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7688191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a quiet valley where the stars shone brighter than anywhere else, there lived a small unicorn named Liora. Her coat was soft and silver, and her horn glowed with the palest blue light, like moonlight on snow.\n",
      "\n",
      "Every unicorn in the valley had a special gift. Some could make flowers bloom with a stomp of their hooves, others could sing songs that made rainbows appear. Liora, however, did not yet know what her gift was.\n",
      "\n",
      "Each night, she watched the older unicorns practice their magic. She tried to help: she stomped her hooves over the grass, but no flowers came. She sang softly to the sky, but no rainbows formed. She dipped her horn into the river, hoping it would turn to sparkling sugar, but the water stayed water.\n",
      "\n",
      "One evening, feeling a little sad, Liora wandered away from the valley and climbed a small hill. Above her, the sky was dark and deep, sprinkled with thousands of tiny stars. The moon hung low and round, like a silver lantern.\n",
      "\n",
      "â€œI wish I knew what Iâ€™m meant to do,â€ she whispered.\n",
      "\n",
      "The wind rustled the grass in answer, gentle and cool. Liora lay down and gazed up, her horn glowing faintly. As she watched, she noticed something new: a star, dim and flickering at the edge of the sky, as if it were too tired to shine.\n",
      "\n",
      "â€œOh,â€ she breathed. â€œYou look lonely, too.â€\n",
      "\n",
      "She stood up and pointed her horn toward the little star. Her horn glowed brighter, then brighter still, sending a slender beam of soft blue light into the sky. The light touched the faint star, wrapping it in a gentle glow.\n",
      "\n",
      "The star quivered, then grew a little stronger. It twinkled back at her, as if saying thank you.\n",
      "\n",
      "Lioraâ€™s heart fluttered. â€œDid I do that?â€\n",
      "\n",
      "She tried again, focusing on another tiny, sleepy star. Her horn warmed and hummed, and once more, a ray of light reached up. The second star brightened, then the first one sparkled even more, as though they were waking each other up.\n",
      "\n",
      "All around her, the sky slowly changed. The dimmest stars began to glow, then to shine, until the whole night seemed to grow deeper and more beautiful. The valley behind her filled with a cool, gentle light, and the other unicorns looked up in wonder.\n",
      "\n",
      "â€œWho is brightening the stars?â€ they murmured.\n",
      "\n",
      "High on the little hill, Lioraâ€™s horn blazed softly like a lantern. She wasnâ€™t forcing the stars to glow; she was simply reminding them of their own light. One by one, they answered her, twinkling awake, until the sky was a blanket of sparkling silver.\n",
      "\n",
      "A soft voice drifted through the night, as if carried on the breeze. â€œLiora,â€ it said, â€œyour gift is to help the smallest lights shine.â€\n",
      "\n",
      "The voice felt like moonlight itself, and Liora understood. She didnâ€™t need to grow flowers or sing rainbows. Her magic was quieter: she made the night less lonely, the darkness less dark, by helping every hidden sparkle find its glow.\n",
      "\n",
      "When her horn finally dimmed, the stars stayed bright. The valley below shimmered with starlight, and the unicorns lay down to sleep, comforted by the soft, steady glow above them.\n",
      "\n",
      "Liora curled up on her hill, the moon smiling down. She no longer wondered what her gift was; she had found it among the stars.\n",
      "\n",
      "And as she closed her eyes, tiny new starsâ€”ones no one had ever noticed beforeâ€”winked into view, shining gently over the world, keeping watch through the quiet, peaceful night.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5.1\",\n",
    "    input=\"Write a short bedtime story about a unicorn.\"\n",
    ")\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266e6b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Responses.create() got an unexpected keyword argument 'frequency_penalty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mresponses\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-5.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a short bedtime story about a unicorn.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m      5\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m      6\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      7\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m      8\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Responses.create() got an unexpected keyword argument 'frequency_penalty'"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5.1\",\n",
    "    input=\"Write a short bedtime story about a unicorn.\",\n",
    "    temperature=0.7, # Creativity Control : 0.0 (more focused) to 1.0 (more creative)\n",
    "    top_p=1.0, # Nucleus Sampling: 0.0 to 1.0: 1.0 means consider all tokens : 0.5 means consider only top 50% probable tokens\n",
    "    max_output_tokens=200,\n",
    "    frequency_penalty=0.3, # Discourage Repetition: 0.0 to 2.0\n",
    "    presence_penalty=0.0 # Encourage New Topics: 0.0 to 2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bc7a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Chat Completion not Chat Response\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a bedtime story about a unicorn\"}],\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.3\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c80f8849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Moonlit Journey of Luna the Unicorn**\n",
      "\n",
      "Once upon a time in a mystical land called Celestia, where the skies shimmered with rainbow colors and the trees whispered secrets to one another, there lived a gentle unicorn named Luna. She had a coat as white as freshly fallen snow and a mane that sparkled like stardust. But what made Luna truly special was her magnificent horn, which glowed softly in the moonlight, casting a silver light over everything around her.\n",
      "\n",
      "Luna loved to wander through the enchanted forest that surrounded her home. Each evening, as the sun dipped below the horizon and painted the sky with hues of orange and pink, she would set off on adventures, exploring hidden meadows filled with flowers that danced in the breeze.\n",
      "\n",
      "One evening, while wandering deeper into the forest than she had ever been before, Luna stumbled upon an ancient oak tree with twisting roots and shimmering leaves. As she approached, she heard a soft sobbing sound coming from beneath its branches. Curious and concerned, she stepped closer to investigate.\n",
      "\n",
      "Beneath the oak sat a small fairy named Tilly. Her wings were damp from tears, and her tiny face was streaked with sadness. \"Oh dear fairy,\" Luna asked gently, \"why do you weep under this beautiful tree?\"\n",
      "\n",
      "Tilly looked up at Luna with big teary eyes. \"I lost my way home during twilight,\" she sniffed, \"and now I can't find my way back to my village before nightfall! If I donâ€™t return soon, my friends will worry.\"\n",
      "\n",
      "Luna's heart swelled with compassion for Tilly. \"Do not worry! I will help you find your way home!\" With that promise made, Luna lowered herself so Tilly could climb onto her back.\n",
      "\n",
      "Together they set off on their adventure through the moonlit forest. The silver light from Lunaâ€™s horn illuminated their path as they galloped past sparkling streams and fields of glowing flowers that lit up like lanterns in the night.\n",
      "\n",
      "As they journeyed deeper into Celestia's enchanted woods, they encountered different magical creatures who offered their help along the wayâ€”wise old owls who shared stories of constellations guiding them northward and playful fireflies who danced around them like tiny stars.\n",
      "\n",
      "After what felt like hours of laughter and friendship under the night sky, they finally reached a clearing where Tilly recognized familiar flowers blooming brightlyâ€”a sign that they were close to her village!\n",
      "\n",
      "\"Thank you for helping me find my way!\" said Tilly joyfully as she fluttered off Lunaâ€™s back and began to twinkle like one of her beloved stars.\n",
      "\n",
      "Just then, Tilly turned back to face her new friend. â€œWait! Let me give you something special!â€ She waved her tiny hands in intricate patterns while whispering an ancient spell; suddenly glowing dust filled the air around them.\n",
      "\n",
      "â€œWhenever you need guidance or wish for adventure,â€ Tilly said with a smile, â€œjust look up at the stars! Theyâ€™ll always point you towards your dreams.â€\n",
      "\n",
      "With hearts full of joy from their newfound friendship and promises kept under starlit skies, they shared one last hug before parting waysâ€”Luna heading back through the enchanting woods while Tilly flitted toward her village where lights twinkled warmly against darkened hills.\n",
      "\n",
      "That night as Luna trotted home beneath countless stars shining brightly above her headâ€”the very same ones that now held secrets of new adventuresâ€”she felt grateful for both friendships made in unexpected places and magic found within simple journeys.\n",
      "\n",
      "And so every evening thereafter when Luna looked up at those twinkling stars glowing just for herâ€”she remembered sweet little Tilly and knew no matter how far apart they might beâ€”they would always share an unbreakable bond woven by kindness throughout forevermore.\n",
      "\n",
      "With dreams swirling like stardust in her heartâ€”and knowing adventure awaited each new dayâ€”Luna closed her eyes under that magnificent moonlightâ€¦ drifting off into peaceful slumber filled with dreams only unicorns can know.\n",
      "\n",
      "And so ends our taleâ€¦ Goodnight little dreamer; may your own adventures begin tonight among those twinkling stars too! âœ¨ðŸŒ™ðŸ¦„\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bacc63",
   "metadata": {},
   "source": [
    "## Test Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4585a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(text='Hi there, this is a regenerated audio sample for testing the OpenAI transcription API. This audio is short, clear, and suitable for your transcription experiments.', logprobs=None, usage=UsageTokens(input_tokens=88, output_tokens=33, total_tokens=121, type='tokens', input_token_details=UsageTokensInputTokenDetails(audio_tokens=88, text_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = open(\"speech_test_regenerated.mp3\", \"rb\")\n",
    "\n",
    "transcript = client.audio.transcriptions.create(\n",
    "    model=\"gpt-4o-transcribe\",\n",
    "    file=audio_file\n",
    ")\n",
    "\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6edfa",
   "metadata": {},
   "source": [
    "# GROQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04303a8",
   "metadata": {},
   "source": [
    "### Set the key in .env\n",
    "### load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bc382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b039b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd14fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there something I can help you with?"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004f96c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since my knowledge cutoff is December 2023, I'll provide information on AI research news from the period leading up to that date. However, please note that some of the information might have changed or been updated since then.\n",
      "\n",
      "Here are some of the latest developments in AI research from the past few months (up to December 2023):\n",
      "\n",
      "1. **Breakthroughs in Large Language Models:** Researchers have made significant progress in developing more efficient and accurate large language models (LLMs). For example, Meta AI's LLaMA model has shown excellent performance in various NLP tasks, and the researchers have also made improvements to the model architecture.\n",
      "\n",
      "2. **Advances in Explainable AI (XAI):** Researchers at Google have made strides in developing more interpretable and explainable AI models. They have created a new tool, Explainable Neural Networks (ENN), which can provide human-interpretable explanations for AI decisions.\n",
      "\n",
      "3. **AI for Cybersecurity:** Researchers at the University of California, Berkeley have developed a new AI-based system for detecting and preventing cyber attacks. The system uses machine learning algorithms to identify and block malicious traffic in real-time.\n",
      "\n",
      "4. **Advances in Reinforcement Learning:** Researchers at DeepMind have made significant breakthroughs in reinforcement learning (RL) techniques. They have developed new RL algorithms that can learn to solve complex tasks, such as robotic manipulation and game playing.\n",
      "\n",
      "5. **Quantum AI Research:** Researchers at the Massachusetts Institute of Technology (MIT) have started exploring the intersection of quantum computing and AI. They have developed new AI algorithms that can run on quantum hardware and potentially solve complex problems more efficiently than classical AI.\n",
      "\n",
      "6. **Ethics in AI Research:** The Association for the Advancement of Artificial Intelligence (AAAI) has released guidelines for addressing the ethics of AI research. The guidelines aim to promote transparency, accountability, and fairness in AI development.\n",
      "\n",
      "Please note that AI research is a rapidly evolving field, and there may be more recent developments that have not been included in this list.\n",
      "\n",
      "Please keep in mind, since my knowledge stopped in December, 2023. AI advancements could be much faster than what was reported."
     ]
    }
   ],
   "source": [
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Share the latest news about AI research from the past month.\"\n",
    "      }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ba2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from ollama) (2.12.4)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/AAI/lib/python3.12/site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae013e7",
   "metadata": {},
   "source": [
    "# deepseek-r1:1.5b Using OLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d87d2ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky, known as coloration or chromacy, is primarily determined by the amount of sunlight reflected on the Earth's surface. Different parts of the sky have varying levels of this reflection because light reflecting off water particles in the atmosphere, such as clouds and mist, can pass through more air molecules than light from objects like buildings or cars that reflects off dust or other particles. This difference in reflection leads to slight variations in the hues we perceive when looking up at the sky. Additionally, atmospheric conditions, including temperature, humidity, and the presence of trace gases like carbon dioxide, influence how the sky appears.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "#print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370be03",
   "metadata": {},
   "source": [
    "# Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbdf6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc581ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Agentic AI refers to artificial intelligence systems that possess the ability to take autonomous actions, make decisions, and interact with the environment to achieve specific goals or tasks. Unlike traditional AI, which often operates under human direction and focuses on tasks like data analysis or pattern recognition, agentic AI can operate independently and adaptively.\\n\\nKey characteristics of agentic AI may include:\\n\\n1. **Autonomy**: The AI can perform tasks and make decisions without human intervention.\\n2. **Goal-oriented behavior**: It is designed to achieve specific objectives or goals, which could involve planning and decision-making.\\n3. **Adaptability**: Agentic AI can learn from its experiences and change its behavior based on new information or feedback from its environment.\\n4. **Interactivity**: These systems can often interact with complex environments or with humans and other agents to achieve their objectives.\\n\\nExamples of agentic AI can range from advanced robotics (such as autonomous drones or self-driving cars) to software agents in complex computational environments (like trading bots in financial markets).\\n\\nThe development and deployment of agentic AI raise various ethical and safety considerations, particularly regarding accountability, decision-making transparency, and the potential for unintended consequences. Organizations and researchers are actively exploring frameworks and guidelines to ensure that agentic AI systems are developed and used in responsible and beneficial ways.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 263, 'prompt_tokens': 23, 'total_tokens': 286, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b547601dbd', 'id': 'chatcmpl-CgusnSgmeatiONwJPioewSz9q07QI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--411c3209-dbcc-48c9-ac03-97491227e3e0-0' usage_metadata={'input_tokens': 23, 'output_tokens': 263, 'total_tokens': 286, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"question\": \"What is Agentic AI?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1781e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad1daaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Transformers are a type of machine that can change the voltage of an electrical current. They are commonly used in power grids to make it easier to transmit electricity over long distances.\\n\\nHere's a simple analogy to help explain how transformers work:\\n\\nImagine you have a big pipe that carries water from a river to a city. The pipe is like a power line that carries electricity from a power plant to a city. But the problem is that the water (or electricity) is flowing too fast and would be too strong for the city's homes and businesses to handle.\\n\\nA transformer is like a special valve that can slow down the water flow (or reduce the voltage) so that it's safe and usable for the city's homes and businesses. This way, the city can use the electricity without it being too strong or too weak.\\n\\nThere are two types of transformers:\\n\\n1. **Step-up transformer**: This type of transformer increases the voltage of the electrical current, like a pump that makes the water flow faster.\\n2. **Step-down transformer**: This type of transformer decreases the voltage of the electrical current, like a valve that slows down the water flow.\\n\\nTransformers are an essential part of modern power grids, making it possible to transmit electricity over long distances and use it safely and efficiently in our homes, businesses, and industries.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 265, 'prompt_tokens': 42, 'total_tokens': 307, 'completion_time': 0.451809233, 'completion_tokens_details': None, 'prompt_time': 0.002040684, 'prompt_tokens_details': None, 'queue_time': 0.052933905, 'total_time': 0.453849917}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--3b01dd0b-e895-4bfe-9798-833217392d61-0' usage_metadata={'input_tokens': 42, 'output_tokens': 265, 'total_tokens': 307}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "response = llm.invoke(\"Explain transformers in simple words.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30faaa",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb5311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain embeddings in 3 bullet points.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in 3 bullet points.\"\n",
    ")\n",
    "\n",
    "print(template.format(topic=\"embeddings\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcbe49aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': 'Apple', 'revenue': '$383B'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Extract company name and revenue from the text below.\n",
    "Return output ONLY in JSON format with keys company, revenue.\n",
    "\n",
    "Text:\n",
    "Apple generated $383B revenue in 2023.\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI()\n",
    "print(parser.parse(model.invoke(prompt).content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11425088",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e29a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314370aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 embedding length: 1536\n",
      "[0.015356769785284996, 0.03452569618821144, 0.03314683213829994, 0.01639767736196518, -0.021115558221936226, -0.0155595438554883, 0.013437173329293728, 0.012734223157167435, -0.018749859184026718, 0.019087815657258034] ...\n",
      "Text 2 embedding length: 1536\n",
      "[-0.027849717065691948, 0.003486940171569586, 0.04356098920106888, 0.0014729317044839263, 0.014623391442000866, -0.01214989647269249, 0.015505146235227585, -0.00042477401439100504, 0.002828486729413271, 0.023143205791711807] ...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Example text list\n",
    "texts = [\n",
    "    \"Retrieval-Augmented Generation (RAG) improves accuracy.\",\n",
    "    \"Embeddings convert text into high-dimensional vectors.\"\n",
    "]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=texts\n",
    ")\n",
    "\n",
    "# Print embeddings\n",
    "for i, emb in enumerate(response.data):\n",
    "    print(f\"Text {i+1} embedding length:\", len(emb.embedding))\n",
    "    print(emb.embedding[:10], \"...\")  # show first 10 numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dc417df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 embedding length: 512\n",
      "[-0.006820265203714371, 0.01068376749753952, -0.022077158093452454, 0.030908022075891495, -0.06736687570810318, -0.03038763254880905, 0.06591609120368958, 0.033778052777051926, -0.00962721835821867, -0.008815093897283077] ...\n",
      "Text 2 embedding length: 512\n",
      "[-0.03966500610113144, -0.028670484200119972, 0.03462023660540581, -0.05522293224930763, 0.05333595722913742, -0.043862566351890564, -0.04640420898795128, -0.005848662927746773, -0.045826561748981476, 0.06434973329305649] ...\n",
      "Text 3 embedding length: 512\n",
      "[-0.005087872035801411, 0.039913903921842575, 0.016241736710071564, -0.00626326072961092, -0.004804299212992191, -0.030938206240534782, 0.03853302821516991, -0.001427111099474132, -0.020976169034838676, 0.027683284133672714] ...\n"
     ]
    }
   ],
   "source": [
    "response = client.embeddings.create(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    input=texts,\n",
    "    dimensions=512\n",
    ")\n",
    "\n",
    "# Print embeddings\n",
    "for i, emb in enumerate(response.data):\n",
    "    print(f\"Text {i+1} embedding length:\", len(emb.embedding))\n",
    "    print(emb.embedding[:10], \"...\")  # show first 10 numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc608dc",
   "metadata": {},
   "source": [
    "#### Size\n",
    "    1. text-embedding-3-large : Default vector size: 3072 dimensions [Large]\n",
    "    2. text-embedding-3-small : 1536 dimensions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cd81210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "268a5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c468265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "- Transfer learning speeds up deep learning training.\n",
      "- RAG improves retrieval accuracy for enterprise systems.\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Step 2. Sample documents\n",
    "docs = [\n",
    "    \"India won its Test match by 200 runs.\",\n",
    "    \"RAG improves retrieval accuracy for enterprise systems.\",\n",
    "    \"Transfer learning speeds up deep learning training.\"\n",
    "]\n",
    "\n",
    "# Step 3. Create vector store\n",
    "faiss_index = FAISS.from_texts(docs, embedding_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64c45063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results:\n",
      "- Transfer learning speeds up deep learning training.\n",
      "- RAG improves retrieval accuracy for enterprise systems.\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Search\n",
    "query = \"How do I improve AI accuracy?\"\n",
    "results = faiss_index.similarity_search(query, k=2)\n",
    "\n",
    "print(\"Top results:\")\n",
    "for r in results:\n",
    "    print(\"-\", r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2813f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb7cd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12a452bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing FAISS index\n",
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35087d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\n",
    "    \"AI can automate repetitive tasks.\",\n",
    "    \"Machine learning models are improving healthcare outcomes.\",\n",
    "    \"The stock market crashed yesterday due to global tension.\",\n",
    "    \"Cats are very playful animals.\",\n",
    "    \"Python is widely used in data science.\",\n",
    "    \"Electric vehicles are becoming more popular.\",\n",
    "    \"The weather today is sunny with mild winds.\",\n",
    "    \"Football is the most popular sport in the world.\",\n",
    "    \"Quantum computing is the future of cryptography.\",\n",
    "    \"Deep learning helps in image recognition.\",\n",
    "    \"The restaurant serves Italian and Mexican food.\",\n",
    "    \"Traveling to Japan is on my wishlist.\",\n",
    "    \"Reinforcement learning trains agents via rewards.\",\n",
    "    \"Natural Language Processing enables chatbots.\",\n",
    "    \"The new iPhone was launched with major upgrades.\",\n",
    "    \"Meditation improves mental health.\",\n",
    "    \"Blockchain technology ensures secure transactions.\",\n",
    "    \"Yoga enhances flexibility and strength.\",\n",
    "    \"Cloud computing allows scalable applications.\",\n",
    "    \"Climate change impacts global ecosystems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49e28f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fedbf86f-e0f3-4aa6-82a8-9438d174773f',\n",
       " '64fba312-22a9-440d-921e-1c029df9593d',\n",
       " '892017d6-430c-4215-abfc-e7dd386749f9',\n",
       " 'bbb04f95-0d53-4a0c-b4b0-dd6813cac7c9',\n",
       " 'dedf16ef-62e1-46af-b8b4-b0a663ca0d31',\n",
       " 'adc02ff3-8444-4583-9306-ab0a636b2f56',\n",
       " '55cfc178-a97c-41a2-9eff-d259ba1d4613',\n",
       " '5e6cb105-5166-4390-8617-86f0ffcfed28',\n",
       " '59cb40b5-4b3e-4d25-a79d-b95128d3afed',\n",
       " '5a585964-b038-4b3b-8a7d-69775f19282d',\n",
       " '511f5dd6-d287-42c1-a5ab-facff615d06f',\n",
       " '31b7c22b-d3ee-486f-aa7e-dc7f1710ef68',\n",
       " 'ecf6ba3d-a2e2-478f-a817-a4dc98ac4e27',\n",
       " '23b4fdaf-88b3-4977-bc0e-34d03675b16a',\n",
       " '2d26d37c-bcb1-40cc-9523-cd000ecea548',\n",
       " 'c3158f71-c5aa-4f3f-99e8-f4661c5e8e8d',\n",
       " '4c805f06-82ca-4078-ae92-e320e5edbaff',\n",
       " 'b0c43edd-4a34-4866-9bea-5e0594b92177',\n",
       " 'abe38cf7-043e-470a-84c2-ed29fa58ddf6',\n",
       " '0a0be554-5325-468e-a1c5-0f40917ec87e']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_texts(docs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26516d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated index\n",
    "db.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb0e8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top search results:\n",
      "- Machine learning models are improving healthcare outcomes.\n",
      "- AI can automate repetitive tasks.\n",
      "- Deep learning helps in image recognition.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can AI help in healthcare?\"\n",
    "results = db.similarity_search(query, k=3)\n",
    "print(\"\\nTop search results:\")\n",
    "for r in results:\n",
    "    print(\"-\", r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a62c4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: RAG improves retrieval accuracy for enterprise systems.\n",
      "Score: 1.3513801\n",
      "---\n",
      "Text: Cloud computing allows scalable applications.\n",
      "Score: 1.5137949\n",
      "---\n",
      "Text: Transfer learning speeds up deep learning training.\n",
      "Score: 1.5689678\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How do vector databases work?\"\n",
    "\n",
    "results = db.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in results:\n",
    "    print(\"Text:\", doc.page_content)\n",
    "    print(\"Score:\", score)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5de74c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index type: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"FAISS index type:\", db.index.metric_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335441d",
   "metadata": {},
   "source": [
    "#### distance\n",
    "1 -- Inner Product (higher is better)\n",
    "\n",
    "0 -- L2 Distance (Smaller is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fab01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b35dfa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric type: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "texts = [\n",
    "    \"India won the cricket match.\",\n",
    "    \"FAISS is used for similarity search.\",\n",
    "    \"Embeddings turn text into vectors.\"\n",
    "]\n",
    "\n",
    "db = FAISS.from_texts(\n",
    "    texts,\n",
    "    emb,\n",
    "    distance_strategy=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "print(\"Metric type:\", db.index.metric_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1968",
   "metadata": {},
   "source": [
    "#### Different Distance\n",
    "    1. DistanceStrategy.EUCLIDEAN (L2)\n",
    "        - L2\n",
    "        - Lower = closer\n",
    "        - Useful for numeric vectors, clustering\n",
    "    2. DistanceStrategy.COSINE\n",
    "        - Equivalent to cosine similarity\n",
    "        - Higher = closer\n",
    "        - Best for RAG, semantic search\n",
    "    3. DistanceStrategy.MAX_INNER_PRODUCT\n",
    "        - Higher = similar\n",
    "        - Good for ranking tasks\n",
    "        - Recommendation Systems (weighted embeddings), Product Ranking on E-commerce Platforms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a20bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "# Cosine similarity\n",
    "db_cos = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.COSINE)\n",
    "\n",
    "# Euclidean distance\n",
    "db_l2 = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.EUCLIDEAN)\n",
    "\n",
    "# Max inner-product\n",
    "db_ip = FAISS.from_texts(texts, emb, distance_strategy=DistanceStrategy.MAX_INNER_PRODUCT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e542d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d60a7ac8",
   "metadata": {},
   "source": [
    "## Effective Prompt\n",
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42593ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regulation does not explicitly cover this point. Section 4.1 specifies reporting requirements for transactions above $10,000 but does not differentiate between domestic and foreign transactions.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI()\n",
    "\n",
    "# Mock RAG retrieved content\n",
    "retrieved_regulation_chunks = \"\"\"\n",
    "Section 4.1: Transactions above $10,000 must be reported within 24 hours.\n",
    "Section 5.3: All foreign transactions require dual authorization.\n",
    "\"\"\"\n",
    "\n",
    "# User question\n",
    "audit_team_question = \"Do we need to report domestic transactions above $15,000?\"\n",
    "\n",
    "# Construct Better Prompt\n",
    "prompt = f\"\"\"\n",
    "You are a Financial Compliance Assistant for internal audit teams.\n",
    "Use ONLY the regulatory text provided in the CONTEXT section.\n",
    "For the given QUESTION, follow these rules:\n",
    "\n",
    "1. Provide a concise answer in 2â€“3 sentences.\n",
    "2. Cite the specific regulation clause numbers or section titles you used.\n",
    "3. If the regulation does not explicitly mention the requested information, state:\n",
    "   \"The regulation does not explicitly cover this point.\"\n",
    "4. Do NOT guess or infer new rules. Do NOT provide advice beyond the context.\n",
    "\n",
    "CONTEXT:\n",
    "{retrieved_regulation_chunks}\n",
    "\n",
    "QUESTION:\n",
    "{audit_team_question}\n",
    "\"\"\"\n",
    "\n",
    "# Invoke LLM\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=prompt,\n",
    "    max_output_tokens=250\n",
    ")\n",
    "\n",
    "# Output\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7637bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, you are not allowed to work from home for 10 days in a month, as the policy permits work-from-home for up to 8 days per month with manager approval (Section 4.1).\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Mock HR policy chunk (normally retrieved via RAG)\n",
    "retrieved_hr_policy_chunks = \"\"\"\n",
    "Section 2.3: Employees can take up to 12 paid sick leaves per calendar year.\n",
    "Section 4.1: Work-from-home is allowed for up to 8 days per month with manager approval.\n",
    "Section 5.4: Overtime pay applies only when weekly work exceeds 40 hours.\n",
    "\"\"\"\n",
    "\n",
    "employee_question = \"Am I allowed to work from home for 10 days in a month?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an Internal HR Policy Assistant.\n",
    "Use ONLY the HR policy text provided in the CONTEXT section.\n",
    "For the given QUESTION, follow these rules:\n",
    "\n",
    "1. Provide a clear answer in 2â€“4 sentences.\n",
    "2. Cite the exact HR policy section numbers used.\n",
    "3. If the policy does not explicitly cover the question, say:\n",
    "   \"The policy does not explicitly cover this issue.\"\n",
    "4. Do NOT invent new HR rules or benefits.\n",
    "\n",
    "CONTEXT:\n",
    "{retrieved_hr_policy_chunks}\n",
    "\n",
    "QUESTION:\n",
    "{employee_question}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=prompt,\n",
    "    max_output_tokens=200\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "824c30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy does not explicitly address this issue.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Mock Security policy chunk (normally from vector DB)\n",
    "retrieved_security_policy_chunks = \"\"\"\n",
    "Control ID SEC-12: All company laptops must use full-disk encryption.\n",
    "Control ID SEC-18: Passwords must be changed every 90 days.\n",
    "Control ID SEC-27: Multi-factor authentication (MFA) is mandatory for VPN access.\n",
    "\"\"\"\n",
    "\n",
    "it_security_question = \"Is multi-factor authentication required for accessing company email?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an Internal Information Security Policy Assistant.\n",
    "Use ONLY the security policy text provided in the CONTEXT section.\n",
    "For the given QUESTION, follow these rules:\n",
    "\n",
    "1. Provide a precise answer in 2â€“4 sentences.\n",
    "2. Cite the exact control IDs you used.\n",
    "3. If the policy does not directly address the question, state:\n",
    "   \"The policy does not explicitly address this issue.\"\n",
    "4. Do NOT invent new security requirements or controls.\n",
    "5. Maintain compliance-ready, factual language.\n",
    "\n",
    "CONTEXT:\n",
    "{retrieved_security_policy_chunks}\n",
    "\n",
    "QUESTION:\n",
    "{it_security_question}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=prompt,\n",
    "    max_output_tokens=200\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
